1. 化繁为简法 (Simplicity is Beauty)
核心逻辑： 挑战“复杂即高级”的偏见。如果前人用了 5 个模块才达到 90% 的精度，你只用 1 个核心模块就达到了 89% 或更高，这本身就是极大的创新。

详解： 寻找冗余。比如在深度学习中，如果大家都用复杂的 Attention 机制，你证明只用简单的线性变换（Linear）或池化（Pooling）也能达到类似效果，这不仅降低了计算量，还揭示了问题的本质。

代表作： ResNet（把复杂的映射简化为残差学习）。

2. 包容并蓄法 (Hybrid/Fusion)
核心逻辑： 1+1 > 2。核心不在于简单的“堆砌”，而在于**“耦合”**。

详解： A 方法速度快但精度低，B 方法精度高但推理慢。你的工作是找到 A 和 B 的结合点（例如：用 A 做粗筛，用 B 做精排），或者把 B 的某种特征提取能力引入到 A 的框架中。

关键： 必须解释清楚为什么这两个东西能结合，解决的是什么共同痛点。

3. 推广应用法 (Generalization/Domain Adaptation)
核心逻辑： 迁移价值。证明一个在 A 领域成功的理论，在 B 领域同样（或更）有效。

详解： 比如将计算机视觉（CV）中的 Transformer 架构引入到语音识别或医疗影像中。创新点在于：针对新领域的特殊性做了哪些适配（Adaptation）。

技巧： 强调“新瓶装旧酒”时，那个“新瓶”对酒的保鲜作用。

4. 弥补不足法 (Problem Definition)
核心逻辑： 查漏补缺。在成熟的赛道中找到前人“刻意回避”或“尚未触达”的边缘场景。

详解： 前人的算法在实验室数据集上表现完美，但在长尾分布、噪声干扰、低功耗设备上表现极差。你定义了这个“不完美场景”，并给出了解决方案。

价值： 这种创新最容易被工业界接受。

5. 天马行空法 (New Task/Paradigm)
核心逻辑： 开辟新赛道。不跟别人比谁跑得快，而是直接定一个新的终点。

详解： 提出一个新的 Benchmark 或一个新的评估维度。例如，过去大家只比准确率，你提出要比“模型的可解释性”或“抗攻击能力”，并为此建立了一套完整的度量标准。

风险： 这种方法难度最大，需要极强的说服力和实验支撑。

6. 解释演绎法 (Insight & Verification)
核心逻辑： 理论先行，实验收尾。不仅仅是“Work”，还要说清楚“为什么 Work”。

详解： 类似物理学研究，通过公式推导或大量的消融实验，发现某个参数或结构才是决定性能的关键，然后基于这个发现设计出更轻量、更高效的方法。

评价： 这是顶会（如 ICML, NeurIPS）最青睐的类型，极具学术深度。

7. 考古翻新法 (Back to the Future)
核心逻辑： 经典永不过时。利用现代的技术积累（算力、数据、训练技巧）重新评估旧方法。

详解： 很多 90 年代的算法因为当时算力不够被埋没了。现在加上 Adam 优化器、Dropout、数据增强等“现代调料”，发现它竟然比去年的 SOTA（最先进技术）还要猛。

代表作： ConvNeXt（证明经过调优的卷积网络依然不输 Transformer）。

8. 挖掘比较法 (Variable Correlation)
核心逻辑： 控制变量。通过大量的对比实验，挖掘出隐藏的影响因子。

详解： 大家都认为 A 导致了 B，但你通过实验发现，其实是变量 C 在起作用，或者 A 只有在 C 的前提下才会导致 B。这种创新在于对**“因果关系”**的重构。
